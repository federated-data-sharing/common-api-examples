{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Analysis - Genomics scenario\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook steps through an scenario of querying federated genomics data using the [Common API](https://github.com/federated-data-sharing/common-api). \n",
    "A task is defined to match SNPs of interest (based on genomic coordinate and mutation). \n",
    "Individual participant/patient/donor data is tested against a list of SNPs of interest. \n",
    "The results are then combined and visualised.\n",
    "\n",
    "- This scenario was inspired by the datasets available at the [International Cancer Genome Consortium](https://dcc.icgc.org/). We acknowledge the generous data sharing policy and all the donors who participated\n",
    "- In order to link SNPs to likely clinical consequences, we build on the work and data provided by [NIH ClinVar](https://www.ncbi.nlm.nih.gov/clinvar/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutations of interest\n",
    "\n",
    "The SNPs of interest are defined in a CSV file locally: [`top_mutations.csv`](./top_mutations.csv).\n",
    "\n",
    "> In this version, the list is fixed and built into the containerised task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "\n",
    "from automate import task_automate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snps = pd.read_csv('top_mutations.csv')\n",
    "snps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the container\n",
    "\n",
    "The task is defined in the python script [`snp-match.py`](./snp-match.py) and a container is used to wrap up the task for execution at remote sites.\n",
    "See also the shell script: [`build-docker-container.sh`](./build-docker-container.sh).\n",
    "\n",
    "Skip this step if you only using a prebuilt container.\n",
    "\n",
    "> It's generally a good idea to run the script locally first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build . -t snp-match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag and push the image to the container registry\n",
    "# Depneds on the ACR_REGISTRY environment variable\n",
    "!sudo docker tag snp-match \"$ACR_REGISTRY/snp-match:latest\"\n",
    "!sudo docker images | grep snp-match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker push \"$ACR_REGISTRY/snp-match:latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing remotely\n",
    "\n",
    "In this scenario, the SNP data is distributed in N (N=3) sites. For simplicity, all nodes have the same API key (token) which is kept in the environment property `FDS_API_TOKEN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'FDS_API_TOKEN' not in os.environ:\n",
    "    print('Please ensure FDS_API_TOKEN is in the environment')\n",
    "else:\n",
    "    print('Found API token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note for now these are the same nodes...\n",
    "# Depends on endpoint URLs e.g. node1 ... node3 at example.org\n",
    "endpoints = [\n",
    "     'https://node1.example.org/v1/api',\n",
    "     'https://node2.example.org/v1/api',\n",
    "     'https://node3.example.org/v1/api',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the task\n",
    "task = {\n",
    "    \"task\":{\n",
    "        \"name\": \"SNP Match\",\n",
    "        \"description\":\"This task looks for SNPs of interest and returns unique donor counts per SNP.\",\n",
    "        \"queryInput\": {\n",
    "            \"selectionQuery\": \"{ snp_clean { donor_id chromosome chromosome_start chromosome_end mutated_from_allele mutated_to_allele } }\"\n",
    "        },\n",
    "        \"container\": {\n",
    "            \"name\":\"snp-match\",\n",
    "            \"tag\":\"latest\",\n",
    "            \"registry\":\"covid19acregistry.azurecr.io\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - move this to top\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# A function to process zip file output from the task and return a dataframe\n",
    "# This is specific to the task specified above\n",
    "def process_zip(path_to_zipfile):\n",
    "    print(f'Processing: {path_to_zipfile}')\n",
    "    with ZipFile(path_to_zipfile) as zipfile:\n",
    "        # TODO - make this more robust\n",
    "        csv_file = zipfile.namelist()[0]\n",
    "        with zipfile.open(csv_file) as csv:\n",
    "            df = pd.read_csv(csv)\n",
    "            return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this section, we run the task at each of the federated endpoints \n",
    "# using Python's concurrency support\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "i = 0\n",
    "with ThreadPoolExecutor(max_workers=len(endpoints)) as executor:\n",
    "    futures = []\n",
    "    for endpoint in endpoints:\n",
    "        i = i + 1\n",
    "        futures.append(executor.submit(task_automate, f'Task-{i}', endpoint, task, process_zip))\n",
    "        time.sleep(1)\n",
    "\n",
    "results = []\n",
    "for f in futures:\n",
    "    x = f.result()\n",
    "    if x is not None:\n",
    "        reference, df = x\n",
    "        # Add a 'first' column\n",
    "        df.insert(0, 'subgroup',reference)\n",
    "        results.append(df)\n",
    "    \n",
    "print(len(results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine results from different pools of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat(results)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
